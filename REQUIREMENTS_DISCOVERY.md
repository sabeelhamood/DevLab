# Requirements Discovery & Analysis - DEVLAB Microservice

## Executive Summary

This document captures comprehensive functional and non-functional requirements for the DEVLAB Microservice, ensuring complete, conflict-free, and TDD-ready specifications aligned with strategic objectives. All requirements are designed to support Test-Driven Development with clear acceptance criteria.

---

## Functional Requirements

### FR-1: Question Generation System

#### FR-1.1: Dynamic Coding Question Generation
**Description**: System must generate coding questions dynamically based on multiple input parameters.

**Input Parameters**:
- `lesson_id` (string, required)
- `course_name` (string, required)
- `lesson_name` (string, required)
- `nano_skills` (array, required) - Array of nano-level skills
- `micro_skills` (array, required) - Array of micro-level skills
- `question_type` (string, required) - "code" or "theoretical"
- `programming_language` (string, required) - One of: Python, Java, JavaScript, C++, Go, Rust
- `quantity` (integer, optional, default: 4) - Number of questions to generate

**Difficulty Distribution**:
- **No difficulty level input required**: System does not accept explicit difficulty level as input parameter
- **Automatic difficulty distribution**: Gemini AI automatically distributes difficulty levels from low to high among all generated questions
- **Difficulty based on skills**: Difficulty is determined by Gemini based on the provided nano_skills and micro_skills complexity
- **Variety ensured**: When generating multiple questions (e.g., quantity: 4), Gemini ensures difficulty progression (easy â†’ medium â†’ hard)

**Process Flow**:
1. Receive request from Content Studio or Assessment microservice
2. Validate input parameters (note: no difficulty level parameter)
3. Call Gemini API with formatted prompt including all context (nano_skills, micro_skills)
4. Gemini generates questions with automatic difficulty distribution (low to high)
5. Receive generated question(s) with test cases and inferred difficulty levels
6. Format response with question text, test cases, style information (including difficulty)
7. Return AJAX response package

**Acceptance Criteria**:
- âœ… Questions generated are relevant to provided nano_skills and micro_skills
- âœ… Difficulty levels automatically distributed from low to high by Gemini
- âœ… Test cases are automatically generated for each question
- âœ… Questions are appropriate for the specified programming language
- âœ… Response includes complete question package (question, test cases, style with difficulty)
- âœ… Generation time < 5 seconds per question
- âœ… Questions are unique and non-repetitive within same lesson context
- âœ… When generating multiple questions, difficulty progression is evident

**TDD Test Cases**:
- Test question generation with valid inputs
- Test question generation with invalid programming language
- Test question generation with missing required parameters
- Test automatic difficulty distribution (verify low to high progression)
- Test difficulty based on nano_skills and micro_skills complexity
- Test response format validation (includes difficulty in style)
- Test Gemini API error handling
- Test test case generation completeness
- Test difficulty progression when generating multiple questions

---

#### FR-1.2: Theoretical Question Routing
**Description**: When question_type is "theoretical", route request to Assessment microservice.

**Process Flow**:
1. Receive request from Content Studio with `question_type: "theoretical"`
2. Validate request parameters
3. Forward request to Assessment microservice API endpoint
4. Wait for Assessment response
5. Validate received theoretical questions
6. Return formatted response to Content Studio

**Acceptance Criteria**:
- âœ… Theoretical requests are correctly identified
- âœ… Requests forwarded to Assessment microservice with all required data
- âœ… Response received from Assessment is validated
- âœ… Error handling when Assessment microservice is unavailable
- âœ… Timeout handling for Assessment API calls

**TDD Test Cases**:
- Test theoretical question routing logic
- Test Assessment microservice integration
- Test error handling for Assessment API failures
- Test request/response validation

---

### FR-2: Code Execution System

#### FR-2.1: Secure Code Execution
**Description**: Execute user-submitted code in secure, isolated sandbox environment using Judge0 API.

**Input Parameters**:
- `code` (string, required) - User's code solution
- `programming_language` (string, required) - Language identifier
- `test_cases` (array, required) - Array of test case objects
- `question_id` (string, required) - Reference to question being solved

**Supported Languages**:
- All languages supported by Judge0 API (see Judge0 documentation for complete list)
- Common languages include: Python, Java, JavaScript, C++, Go, Rust, C#, PHP, Ruby, Swift, Kotlin, and many more
- Language support is determined by Judge0 API capabilities, not limited to a specific subset
- System must dynamically support any language that Judge0 API supports

**Process Flow**:
1. Validate code input and sanitize if necessary
2. Format code execution request for Judge0 API
3. Submit execution request to Judge0
4. Poll for execution results (handle async execution)
5. Parse execution results (output, errors, test case results)
6. Return formatted results to frontend

**Security Requirements**:
- âœ… Code execution isolated in sandbox (no access to server)
- âœ… Resource limits enforced (time, memory, CPU)
- âœ… Network access restricted for security
- âœ… File system access restricted
- âœ… Timeout handling for long-running code

**Acceptance Criteria**:
- âœ… Code executes successfully in Judge0 sandbox
- âœ… Execution results correctly parsed and returned
- âœ… Compilation errors are captured and reported
- âœ… Runtime errors are captured with stack traces
- âœ… Test case results are accurate
- âœ… Execution time < 5 seconds for standard programs
- âœ… Timeout errors handled gracefully
- âœ… Security violations blocked and logged

**TDD Test Cases**:
- Test code execution for multiple supported languages (verify Judge0 integration)
- Test compilation error handling
- Test runtime error handling
- Test test case validation
- Test timeout handling
- Test security violation detection
- Test result parsing accuracy
- Test language detection and mapping to Judge0 language IDs

---

#### FR-2.2: Syntax Highlighting Support
**Description**: Provide syntax highlighting for all supported programming languages in code editor.

**Acceptance Criteria**:
- âœ… Syntax highlighting works for all Judge0-supported languages
- âœ… Highlighting updates in real-time as user types
- âœ… Theme-compatible (works in day/night mode)
- âœ… Performance: no lag during typing
- âœ… Language detection based on Judge0 language mappings

---

### FR-3: Intelligent Feedback System

#### FR-3.1: AI-Generated Feedback for Solutions
**Description**: Generate intelligent feedback for user solutions using Gemini AI, regardless of correctness.

**Input Parameters**:
- `code` (string, required) - User's solution code
- `question_context` (object, required) - Question details, expected solution, test cases
- `execution_results` (object, required) - Judge0 execution results
- `is_correct` (boolean, required) - Whether solution passed all test cases

**Feedback Types**:

**For Correct Solutions**:
- Acknowledge correctness
- Provide improvement suggestions (code quality, efficiency, best practices)
- Suggest alternative approaches
- Highlight strengths in the solution
- Encourage further learning

**For Incorrect Solutions**:
- Indicate solution is incorrect
- Provide guidance without revealing answer
- Point to specific issues in code (logic errors, syntax issues)
- Suggest debugging approaches
- Provide hints toward solution direction

**Acceptance Criteria**:
- âœ… Feedback generated for both correct and incorrect solutions
- âœ… Feedback is educational and encouraging
- âœ… Feedback does not reveal solution directly for incorrect answers
- âœ… Feedback includes specific code references when relevant
- âœ… Feedback generation time < 3 seconds
- âœ… Feedback is contextually relevant to question and user's code
- âœ… Feedback is personalized based on common error patterns

**TDD Test Cases**:
- Test feedback generation for correct solutions
- Test feedback generation for incorrect solutions
- Test feedback quality (no answer leakage)
- Test Gemini API error handling
- Test feedback response time
- Test feedback personalization

---

#### FR-3.2: Progressive Hint System
**Description**: Provide 3 progressive hints per question, revealed one at a time. All hints are generated in a single Gemini API call for performance optimization.

**Hint Characteristics**:
- **Hint 1**: Very general direction, minimal information
- **Hint 2**: More specific guidance, points to relevant concepts
- **Hint 3**: Direct guidance toward solution approach (but not the answer itself)
- All hints are short, focused, and non-revealing

**Process Flow**:
1. User requests first hint (clicks "Get Hint" button)
2. System calls Gemini API **once** to generate all 3 hints in single response
3. Store all 3 hints in database (prevent regeneration)
4. Display Hint 1 to user
5. User requests Hint 2 (already stored, retrieve from database)
6. Display Hint 2 to user
7. User requests Hint 3 (already stored, retrieve from database)
8. Display Hint 3 to user
9. Disable "Get Hint" button after 3 hints displayed
10. Enable "View Answer" button after 3 hints displayed

**Performance Optimization**:
- âœ… All 3 hints generated in single Gemini API call (faster response time)
- âœ… Hints stored immediately after first API call
- âœ… Subsequent hint requests use cached hints (no additional API calls)
- âœ… User experiences faster hint delivery (Hint 1 appears quickly, subsequent hints instant)

**Acceptance Criteria**:
- âœ… All three hints generated in single Gemini API response
- âœ… Three unique hints per question
- âœ… Hints are progressively more helpful
- âœ… Hints do not reveal final solution
- âœ… Hints are stored in database after first generation
- âœ… Subsequent hint requests retrieve from database (no API calls)
- âœ… "View Answer" only available after all 3 hints displayed
- âœ… Hints are short (1-2 sentences max)
- âœ… Initial hint response time < 3 seconds (subsequent hints instant)

**TDD Test Cases**:
- Test single API call generates all 3 hints
- Test hint storage after first API call
- Test hint retrieval from database (no API calls for hints 2 and 3)
- Test hint progression logic
- Test "View Answer" button state management
- Test hint uniqueness
- Test hint quality (non-revealing)
- Test performance (single API call vs multiple calls)

---

#### FR-3.3: Solution Viewing
**Description**: Allow learners to view AI-generated solution after using all 3 hints.

**Process Flow**:
1. User clicks "View Answer" button (only enabled after 3 hints used)
2. Call Gemini API to generate optimal solution
3. Format solution with clear explanation
4. Display solution with syntax highlighting
5. Log solution view event (analytics)

**Acceptance Criteria**:
- âœ… Solution only viewable after 3 hints used
- âœ… Solution includes clear code explanation
- âœ… Solution demonstrates best practices
- âœ… Solution is well-commented
- âœ… View event is logged for analytics

**TDD Test Cases**:
- Test "View Answer" button availability logic
- Test solution generation quality
- Test solution viewing event logging

---

#### FR-3.4: AI Fraud Detection
**Description**: Detect if submitted code solutions are human-written or AI-generated using Gemini AI pattern recognition.

**Purpose**: 
- Ensure academic integrity in learning exercises
- Identify potential cheating or unauthorized AI assistance
- Maintain fair competition in anonymous matches
- Validate authentic learning progress

**Detection Process**:
1. User submits code solution
2. Before providing feedback, system calls Gemini API for fraud detection
3. Gemini analyzes code for AI generation patterns:
   - Code structure patterns (AI models often have characteristic patterns)
   - Comment style and placement
   - Variable naming conventions
   - Error handling patterns
   - Code organization and formatting
4. Receive fraud detection score from Gemini
5. Store detection result with submission
6. Take appropriate action based on detection level

**Detection Criteria** (Analyzed by Gemini):
- **Code Patterns**: AI-generated code often exhibits specific structural patterns
- **Comment Quality**: AI-generated comments may be overly perfect or generic
- **Variable Naming**: Human-written code may have more varied naming styles
- **Error Handling**: AI-generated code often has comprehensive error handling
- **Code Completeness**: AI-generated code is often immediately complete without iterations
- **Consistency**: AI code may be too consistent or perfect for the learner's skill level

**Fraud Score Levels**:
- **Low (0-30%)**: Likely human-written, proceed normally
- **Medium (31-60%)**: Suspicious, flag for review, provide warning to learner
- **High (61-90%)**: Likely AI-generated, restrict feedback, log incident
- **Very High (91-100%)**: Confirmed AI-generated, block submission, require re-attempt

**Actions Based on Detection Level**:

**Low Risk (0-30%)**:
- âœ… Normal feedback provided
- âœ… No restrictions applied
- âœ… Submission accepted

**Medium Risk (31-60%)**:
- âš ï¸ Warning message displayed to learner
- âš ï¸ Submission accepted but flagged
- âš ï¸ Flagged for instructor review (if applicable)
- âš ï¸ Limited feedback provided
- âš ï¸ Incident logged in database

**High Risk (61-90%)**:
- ðŸ”´ Submission restricted
- ðŸ”´ Feedback limited or disabled
- ðŸ”´ Incident logged with full details
- ðŸ”´ Notification sent to instructors/trainers
- ðŸ”´ Learner warned about academic integrity
- ðŸ”´ May require manual review before acceptance

**Very High Risk (91-100%)**:
- ðŸš« Submission blocked
- ðŸš« No feedback provided
- ðŸš« Incident logged and escalated
- ðŸš« Automatic notification to administrators
- ðŸš« Learner required to re-attempt with explanation
- ðŸš« May result in competition disqualification

**Integration Points**:
- **Practice Questions**: Fraud detection before feedback generation
- **Competitions**: Fraud detection before winner determination
- **Assessments**: Fraud detection before grading (if integrated)
- **Progress Tracking**: Fraud detection affects learning analytics accuracy

**Privacy and Transparency**:
- âœ… Learners informed about fraud detection system (transparency)
- âœ… Detection logic explained in privacy policy
- âœ… False positives handled with appeal process
- âœ… Detection results stored securely (MongoDB Atlas)
- âœ… Audit trail maintained for all detections

**Acceptance Criteria**:
- âœ… Fraud detection runs automatically on all code submissions
- âœ… Detection uses Gemini AI pattern recognition
- âœ… Fraud score calculated accurately (0-100%)
- âœ… Appropriate actions taken based on risk level
- âœ… Incidents logged in database for audit
- âœ… False positive rate minimized (< 5%)
- âœ… Detection time < 2 seconds (does not significantly impact user experience)
- âœ… Privacy maintained (learners informed, data secured)

**TDD Test Cases**:
- Test fraud detection for human-written code (low risk)
- Test fraud detection for AI-generated code (high risk)
- Test fraud detection for mixed code (medium risk)
- Test fraud detection response time
- Test action enforcement based on risk level
- Test incident logging and audit trail
- Test false positive handling
- Test integration with feedback system
- Test integration with competition system

---

### FR-4: Competition System

#### FR-4.1: Competition Invitation Creation
**Description**: Create competition invitation when learner completes a course (triggered by Course Builder).

**Input from Course Builder**:
- `course_id` (string, required)
- `learner_id` (string, required)

**Process Flow**:
1. Receive completion notification from Course Builder
2. Query database for learner's skill level
3. Find multiple matching learners at similar skill level who completed same course
4. Create competition invitation pool with list of potential competitors
5. Store competition invitation record in DEVLAB database with status: "pending"
6. Display invitation to learner with **anonymous list of potential competitors**
7. Learner can view list of other learners who received same invitation (names hidden, IDs shown as anonymous identifiers)
8. Learner selects one competitor from the list
9. Selected competitor receives notification to accept/decline
10. Once both learners accept, competition begins

**Competitor Selection Interface**:
- Learner sees list of anonymous identifiers (e.g., "Competitor #1", "Competitor #2")
- Each identifier shows: skill level match, course completion status, availability status
- Learner can select preferred competitor from the list
- Names and personal information remain hidden (full anonymity)

**Matching Algorithm**:
- Match learners with same `course_id` completion
- Match by similar skill level (Â±1 level acceptable)
- Match learners who haven't competed against each other recently
- Match learners who are available (not already in active competition)
- Generate pool of 3-5 potential competitors per invitation

**Acceptance Criteria**:
- âœ… Invitation created when Course Builder sends completion event
- âœ… Multiple potential competitors found and stored
- âœ… Learner can view anonymous list of potential competitors
- âœ… Learner can select preferred competitor from list
- âœ… Competition invitation stored in DEVLAB (not sent to other services)
- âœ… Full anonymity maintained (no names revealed)
- âœ… Duplicate competitions prevented
- âœ… Handles case when no match available
- âœ… Selected competitor receives acceptance request

**TDD Test Cases**:
- Test invitation creation on course completion
- Test matching algorithm accuracy
- Test duplicate prevention
- Test no-match scenario handling
- Test competition invitation data structure

---

#### FR-4.1.1: Competitor Selection Interface
**Description**: Allow learners to view and select from anonymous list of potential competitors.

**UI Components**:
- Competition invitation card showing course completion
- "View Available Competitors" button
- Modal/panel showing list of anonymous competitors
- Each competitor displayed with:
  - Anonymous identifier (e.g., "Competitor #1")
  - Skill level match indicator
  - Course completion badge
  - Availability status
  - "Select" button

**Acceptance Criteria**:
- âœ… List displays anonymous identifiers only (no names)
- âœ… Competitor selection is intuitive and clear
- âœ… Selected competitor is notified for acceptance
- âœ… UI maintains privacy and anonymity

---

#### FR-4.2: Competition Execution
**Description**: Execute anonymous competition between two matched learners with 3 coding questions.

**Competition Structure**:
- **Questions**: 3 coding questions (generated dynamically)
- **Time Limit**: Per question or total competition time (TBD)
- **Evaluation**: Solutions sent to Gemini for comparison and winner determination

**Process Flow**:
1. Both learners accept competition invitation
2. Generate 3 coding questions using Gemini (appropriate for course and skill level)
3. Present questions sequentially or simultaneously
4. Learners submit solutions for each question
5. Execute solutions via Judge0
6. Send all solutions and questions to Gemini for evaluation
7. Determine winner based on Gemini evaluation
8. Store competition results
9. Send performance data to Learning Analytics

**Winner Determination Criteria** (via Gemini):
- Correctness of solutions
- Code quality and best practices
- Efficiency and optimization
- Readability and maintainability
- Test case coverage

**Acceptance Criteria**:
- âœ… 3 questions generated for competition
- âœ… Questions are appropriate difficulty for skill level
- âœ… Solutions executed correctly via Judge0
- âœ… Winner determined accurately via Gemini
- âœ… Competition results stored in database
- âœ… Performance data sent to Learning Analytics
- âœ… Competition is anonymous (identities not revealed)
- âœ… Game elements enhance engagement

**TDD Test Cases**:
- Test question generation for competitions
- Test competition flow execution
- Test solution evaluation logic
- Test winner determination accuracy
- Test Learning Analytics integration
- Test anonymity maintenance

---

#### FR-4.3: Competition Performance Reporting
**Description**: Send competition performance data to Learning Analytics microservice.

**Data Sent**:
```json
{
  "learner1_id": "string",
  "learner2_id": "string",
  "course_id": "string",
  "performance_learner1": {
    "score": number,
    "questions_correct": number,
    "code_quality_score": number,
    "time_taken": number,
    "rank": number
  },
  "performance_learner2": {
    "score": number,
    "questions_correct": number,
    "code_quality_score": number,
    "time_taken": number,
    "rank": number
  },
  "competition_id": "string",
  "timestamp": "ISO 8601 datetime"
}
```

**Process Flow**:
1. Competition completes
2. Calculate performance metrics for both learners
3. Format data according to Learning Analytics API spec
4. Send POST request to Learning Analytics endpoint
5. Handle response and log status

**Acceptance Criteria**:
- âœ… Performance data calculated accurately
- âœ… Data formatted correctly for Learning Analytics
- âœ… API call succeeds and handles errors
- âœ… Data sent immediately after competition completion
- âœ… Retry logic for failed API calls

**TDD Test Cases**:
- Test performance calculation accuracy
- Test data formatting
- Test Learning Analytics API integration
- Test error handling and retry logic

---

### FR-4.4: AI Fraud Detection in Competitions
**Description**: Apply fraud detection to competition submissions to ensure fair play.

**Special Considerations for Competitions**:
- âœ… All competition submissions automatically checked for AI generation
- âœ… High-risk detections result in automatic disqualification
- âœ… Both competitors' submissions checked independently
- âœ… Fraud detection results factored into winner determination
- âœ… Disqualifications logged and reported to Learning Analytics

---

### FR-5: Content Validation System

#### FR-5.1: Trainer Question Validation
**Description**: Validate trainer-submitted questions for relevance and quality using Gemini AI.

**Input from Content Studio**:
- `question` (string, required) - Trainer's question text/code
- `lesson_id` (string, required)
- `course_name` (string, required)
- `lesson_name` (string, required)
- `nano_skills` (array, required)
- `micro_skills` (array, required)
- `question_type` (string, required) - "code" or "theoretical"
- `programming_language` (string, nullable) - Required if question_type is "code"

**Validation Criteria**:
- Relevance to course objectives
- Alignment with nano_skills and micro_skills
- Question clarity and comprehensibility
- Appropriate difficulty level
- Code correctness (if coding question)
- Test case completeness (if coding question)
- Uniqueness (not duplicate of existing questions)

**Process Flow**:
1. Receive validation request from Content Studio
2. Extract question and context data
3. Call Gemini API with validation prompt
4. Receive validation assessment
5. Parse validation result (score, feedback, suggestions)
6. Return validation response to Content Studio

**Response Format**:
```json
{
  "is_valid": boolean,
  "relevance_score": number, // 0-100
  "quality_score": number, // 0-100
  "feedback": string,
  "suggestions": array, // Array of improvement suggestions
  "issues": array // Array of identified issues
}
```

**Acceptance Criteria**:
- âœ… Question relevance accurately assessed
- âœ… Quality score calculated appropriately
- âœ… Constructive feedback provided
- âœ… Suggestions are actionable
- âœ… Validation time < 5 seconds
- âœ… Response format is consistent

**TDD Test Cases**:
- Test validation for high-quality questions
- Test validation for low-quality questions
- Test relevance scoring accuracy
- Test feedback quality
- Test Gemini API integration
- Test error handling

---

### FR-6: Integration Requirements

#### FR-6.1: Course Builder Integration
**Endpoint**: `POST /api/competitions/invite`

**Request Format**:
```json
{
  "course_id": "string",
  "learner_id": "string"
}
```

**Response Format**:
```json
{
  "success": boolean,
  "invitation_id": "string",
  "message": "string"
}
```

**Acceptance Criteria**:
- âœ… Endpoint accepts Course Builder requests
- âœ… Competition invitation created and stored
- âœ… Response confirms invitation creation
- âœ… Error handling for invalid inputs

---

#### FR-6.2: Content Studio Integration

**Endpoint 1 - Practice Question Creation**: `POST /api/questions/generate`

**Request Format**:
```json
{
  "quantity": 4,
  "lesson_id": "string",
  "course_name": "string",
  "lesson_name": "string",
  "nano_skills": ["string"],
  "micro_skills": ["string"],
  "question_type": "code" | "theoretical",
  "programming_language": "Python" | "Java" | "JavaScript" | "C++" | "Go" | "Rust"
}
```

**Response Format** (AJAX with style):
```json
{
  "success": true,
  "questions": [
    {
      "question_id": "string",
      "question_text": "string",
      "test_cases": [
        {
          "input": "string",
          "expected_output": "string",
          "is_hidden": boolean
        }
      ],
      "programming_language": "string",
      "style": {
        "difficulty": "string",
        "tags": ["string"],
        "estimated_time": number
      }
    }
  ]
}
```

**Endpoint 2 - Trainer Question Validation**: `POST /api/questions/validate`

**Request Format**:
```json
{
  "question": "string",
  "lesson_id": "string",
  "course_name": "string",
  "lesson_name": "string",
  "nano_skills": ["string"],
  "micro_skills": ["string"],
  "question_type": "code" | "theoretical",
  "programming_language": "string" | null
}
```

**Response Format**:
```json
{
  "is_valid": boolean,
  "relevance_score": number,
  "quality_score": number,
  "feedback": "string",
  "suggestions": ["string"],
  "issues": ["string"]
}
```

**Acceptance Criteria**:
- âœ… Both endpoints handle requests correctly
- âœ… Response format includes all required fields
- âœ… AJAX responses include style information
- âœ… Error handling for all failure scenarios

---

#### FR-6.3: Assessment Microservice Integration

**Endpoint - Coding Question Creation**: `POST /api/assessment/questions/generate`

**Request Format**:
```json
{
  "topic_name": "string",
  "programming_language": "string",
  "quantity": number,
  "nano_skills": ["string"],
  "micro_skills": ["string"]
}
```

**Response Format** (NO solution, includes test cases):
```json
{
  "success": true,
  "questions": [
    {
      "question_id": "string",
      "question_text": "string",
      "test_cases": [
        {
          "input": "string",
          "expected_output": "string"
        }
      ],
      "programming_language": "string",
      "style": {
        "difficulty": "string",
        "tags": ["string"]
      }
      // Note: solution NOT included
    }
  ]
}
```

**Process Flow**:
1. Receive request from Assessment
2. Generate questions via Gemini
3. Generate test cases (included in response)
4. Execute test setup in Judge0 (prepare sandbox)
5. Return questions with test cases (NO solution)
6. Assessment will check solutions in their backend

**Theoretical Question Creation**:
- When Content Studio requests theoretical questions, DEVLAB forwards to Assessment
- Assessment creates theoretical questions
- DEVLAB uses Gemini to validate theoretical question solutions when needed

**Acceptance Criteria**:
- âœ… Questions generated without solutions
- âœ… Test cases included in response
- âœ… Judge0 sandbox prepared for assessment use
- âœ… Theoretical question routing works correctly

---

#### FR-6.4: Learning Analytics Integration
**Endpoint**: `POST /api/analytics/competition`

**Request Format** (sent by DEVLAB):
```json
{
  "learner1_id": "string",
  "learner2_id": "string",
  "course_id": "string",
  "performance_learner1": {
    "score": number,
    "questions_correct": number,
    "code_quality_score": number,
    "time_taken": number,
    "rank": 1 | 2
  },
  "performance_learner2": {
    "score": number,
    "questions_correct": number,
    "code_quality_score": number,
    "time_taken": number,
    "rank": 1 | 2
  },
  "competition_id": "string"
}
```

**Acceptance Criteria**:
- âœ… Data sent after competition completion
- âœ… Performance metrics calculated accurately
- âœ… API call succeeds with proper error handling
- âœ… Retry logic for failed calls

---

#### FR-6.5: RAG Microservice Integration
**Description**: Integration with RAG microservice for customer support chatbot.

**Acceptance Criteria**:
- âœ… Chatbot can access DEVLAB context
- âœ… Integration supports conversational queries
- âœ… Response format compatible with chatbot UI

---

## Non-Functional Requirements

### NFR-1: Performance Requirements

#### NFR-1.1: Response Time
- **Question Generation**: < 5 seconds per question
- **Code Execution**: < 5 seconds for standard programs
- **Feedback Generation**: < 3 seconds
- **API Endpoint Response**: < 2 seconds for standard operations
- **Database Queries**: < 500ms for standard queries

#### NFR-1.2: Throughput
- Support **10,000+ concurrent users**
- Handle **100+ code executions per second**
- Process **1000+ question generations per day**
- Support **50+ competition matches simultaneously**

#### NFR-1.3: Scalability
- Auto-scaling based on load
- Multi-region deployment capability
- Horizontal scaling support
- Database connection pooling

---

### NFR-2: Security Requirements

#### NFR-2.1: Code Execution Security
- âœ… Secure sandbox isolation (Judge0)
- âœ… Resource limits (time, memory, CPU)
- âœ… Network access restrictions
- âœ… File system access restrictions
- âœ… Malicious code detection and blocking

#### NFR-2.2: API Security
- âœ… API key authentication for microservices
- âœ… JWT token validation for user requests
- âœ… CORS configuration for frontend-backend
- âœ… Input validation and sanitization
- âœ… Rate limiting per IP/user
- âœ… HTTPS/TLS for all communications

#### NFR-2.3: Data Security
- âœ… Encryption at rest (database)
- âœ… Encryption in transit (HTTPS)
- âœ… PII anonymization for competitions
- âœ… Secure secret management (environment variables)
- âœ… Audit logging for security events

#### NFR-2.4: Cheating Detection and AI Fraud Detection
- âœ… AI-based pattern recognition (via Gemini) for AI-generated code detection
- âœ… Suspicious activity monitoring
- âœ… Solution similarity detection between submissions
- âœ… Time-based anomaly detection
- âœ… Automatic fraud scoring (0-100%) for all code submissions
- âœ… Risk-based actions (warning, restriction, blocking)
- âœ… Incident logging and audit trail
- âœ… False positive mitigation and appeal process

---

### NFR-3: Reliability Requirements

#### NFR-3.1: Availability
- **Target Uptime**: 99.9% availability
- **Scheduled Maintenance Window**: Off-peak hours with advance notice
- **Failover Capability**: Automatic failover to backup systems

#### NFR-3.2: Error Handling
- âœ… Graceful degradation when external APIs unavailable
- âœ… Retry logic for failed API calls (exponential backoff)
- âœ… User-friendly error messages
- âœ… Comprehensive error logging
- âœ… Alert system for critical errors

#### NFR-3.3: Data Integrity
- âœ… Transaction support for critical operations
- âœ… Data validation before storage
- âœ… Backup and recovery procedures
- âœ… Data consistency checks

---

### NFR-4: Usability Requirements

#### NFR-4.1: User Experience
- âœ… Intuitive UI/UI following provided style guide
- âœ… Responsive design (mobile, tablet, desktop)
- âœ… Day/night mode theme support
- âœ… Accessibility features (WCAG 2.1 AA compliance)
- âœ… Loading states and progress indicators
- âœ… Clear error messages and help text

#### NFR-4.2: Accessibility
- âœ… Screen reader support
- âœ… Keyboard navigation
- âœ… Color blind friendly design
- âœ… High contrast mode
- âœ… Large font mode
- âœ… Reduced motion support

---

### NFR-5: Compatibility Requirements

#### NFR-5.1: Browser Support
- Chrome (latest 2 versions)
- Firefox (latest 2 versions)
- Safari (latest 2 versions)
- Edge (latest 2 versions)

#### NFR-5.2: Device Support
- Desktop (1920px+)
- Laptop (1366px - 1919px)
- Tablet (768px - 1365px)
- Mobile (320px - 767px)

---

### NFR-6: Compliance Requirements

#### NFR-6.1: Data Privacy
- âœ… GDPR compliance for learner data
- âœ… Data retention policies
- âœ… Right to deletion support
- âœ… Privacy policy compliance

#### NFR-6.2: Logging and Audit
- âœ… Comprehensive audit trails
- âœ… Security event logging
- âœ… Performance metric logging
- âœ… User action logging (anonymized)

---

## Business Rules

### BR-1: Question Generation Rules
1. Questions must align with specified nano_skills and micro_skills
2. Questions must be appropriate for specified programming language
3. Questions must have minimum 3 test cases (2 public, 1 hidden)
4. Questions cannot be duplicates within same lesson context

### BR-2: Competition Rules
1. Learners can only compete with peers at similar skill level (Â±1 level)
2. Learners cannot compete with same partner within 30 days
3. Competition requires acceptance from both learners
4. Competition questions are generated dynamically (not pre-stored)
5. Winner determined by Gemini evaluation (not just correctness)

### BR-3: Hint and Solution Rules
1. Maximum 3 hints per question
2. Hints must be revealed sequentially (cannot skip)
3. "View Answer" only available after all 3 hints used
4. Hints must not reveal final solution
5. Solutions shown must demonstrate best practices

### BR-4: Code Execution Rules
1. Maximum execution time: 5 seconds per code run
2. Maximum memory: 256MB per execution
3. Maximum file size: 1MB per submission
4. Network access: Restricted (no external API calls)
5. File system: Read-only except temp directory

### BR-5: Data Retention Rules
1. Competition data retained for 1 year
2. Question attempts retained for 6 months
3. Error logs retained for 90 days
4. Performance analytics retained indefinitely (anonymized)

---

## User Stories

### US-1: As a Learner, I want to practice coding questions
**Story**: As a learner, I want to receive personalized coding practice questions based on my current course, lesson, and skill level, so that I can improve my programming skills.

**Acceptance Criteria**:
- âœ… Questions are tailored to my course and lesson
- âœ… Questions match my skill level
- âœ… Questions target specific nano_skills and micro_skills
- âœ… Questions are in the programming language I'm learning
- âœ… I receive 4 questions at a time (configurable)

**TDD Test Cases**:
- Test question personalization logic
- Test skill level matching
- Test question generation for specific course/lesson

---

### US-2: As a Learner, I want intelligent feedback on my solutions
**Story**: As a learner, I want to receive AI-generated feedback on my coding solutions, whether correct or incorrect, so that I can learn and improve my coding skills.

**Acceptance Criteria**:
- âœ… Feedback provided for both correct and incorrect solutions
- âœ… Correct solutions receive improvement suggestions
- âœ… Incorrect solutions receive guidance (not answers)
- âœ… Feedback is specific to my code
- âœ… Feedback is encouraging and educational

**TDD Test Cases**:
- Test feedback generation for correct solutions
- Test feedback generation for incorrect solutions
- Test feedback quality and relevance

---

### US-3: As a Learner, I want progressive hints when stuck
**Story**: As a learner, I want to receive 3 progressive hints when I'm stuck on a question, so that I can learn without getting the answer immediately.

**Acceptance Criteria**:
- âœ… I can request up to 3 hints per question
- âœ… Hints are progressively more helpful
- âœ… Hints don't reveal the final solution
- âœ… After 3 hints, I can view the full solution
- âœ… Hints are short and focused

**TDD Test Cases**:
- Test hint generation for each level
- Test hint progression logic
- Test "View Answer" availability after 3 hints

---

### US-4: As a Learner, I want to compete anonymously with peers
**Story**: As a learner, I want to participate in anonymous coding competitions with peers at my skill level, so that I can challenge myself and stay engaged.

**Acceptance Criteria**:
- âœ… I receive competition invitations after completing courses
- âœ… Competitions are anonymous (identities hidden)
- âœ… Competitions match me with peers at similar skill level
- âœ… Competitions have 3 coding questions
- âœ… Winner is determined fairly by AI evaluation
- âœ… Competition results are logged for analytics

**TDD Test Cases**:
- Test competition invitation creation
- Test matching algorithm
- Test competition execution flow
- Test winner determination

---

### US-5: As a Trainer, I want to submit questions for validation
**Story**: As a trainer, I want to submit custom questions for AI validation, so that I can ensure my questions are relevant and high quality.

**Acceptance Criteria**:
- âœ… I can submit coding or theoretical questions
- âœ… Questions are validated for relevance to course
- âœ… I receive quality scores and feedback
- âœ… I receive actionable improvement suggestions
- âœ… Validation is quick (< 5 seconds)

**TDD Test Cases**:
- Test question validation endpoint
- Test validation accuracy
- Test feedback quality

---

### US-6: As Content Studio, I want to request practice questions
**Story**: As the Content Studio microservice, I want to request practice questions for learners, so that I can provide personalized learning content.

**Acceptance Criteria**:
- âœ… I can request coding or theoretical questions
- âœ… I receive formatted question packages with style
- âœ… Questions are tailored to lesson context
- âœ… Response format is consistent and complete

**TDD Test Cases**:
- Test Content Studio API integration
- Test response format validation
- Test question package completeness

---

## Integration Requirements & Dependencies

### External API Dependencies

#### Gemini API
- **Purpose**: Question generation, feedback, validation, competition evaluation
- **Authentication**: API key (stored in environment variable)
- **Rate Limits**: To be determined (monitor usage)
- **Error Handling**: Retry with exponential backoff
- **Fallback Strategy**: Queue requests if rate limited

#### Judge0 API
- **Purpose**: Secure code execution
- **Authentication**: API key (stored in environment variable)
- **Rate Limits**: Based on Judge0 plan
- **Error Handling**: Retry for transient errors
- **Fallback Strategy**: Queue executions if rate limited

### Microservice Dependencies

#### Course Builder
- **Integration Type**: REST API
- **Authentication**: Service API key
- **Data Flow**: Receives course completion events
- **Criticality**: Medium (competitions are optional feature)

#### Content Studio
- **Integration Type**: REST API
- **Authentication**: Service API key
- **Data Flow**: Receives question generation requests, sends responses
- **Criticality**: High (core functionality)

#### Assessment
- **Integration Type**: REST API (two-way)
- **Authentication**: Service API key
- **Data Flow**: 
  - Receives coding question requests
  - Sends theoretical question requests (forwarded from Content Studio)
  - Validates theoretical solutions
- **Criticality**: High (core functionality)

#### Learning Analytics
- **Integration Type**: REST API
- **Authentication**: Service API key
- **Data Flow**: Sends competition performance data
- **Criticality**: Low (analytics, non-blocking)

#### RAG Microservice
- **Integration Type**: REST API (chatbot)
- **Authentication**: Service API key
- **Data Flow**: Provides customer support context
- **Criticality**: Low (support feature)

---

## Compliance, Regulatory & Security Requirements

### Data Privacy (GDPR Compliance)
- âœ… Learner data anonymized in competitions
- âœ… Right to deletion support
- âœ… Data retention policies enforced
- âœ… Privacy policy compliance

### Security Standards
- âœ… OWASP Top 10 compliance
- âœ… Secure coding practices
- âœ… Regular security audits
- âœ… Vulnerability scanning

### Audit and Logging
- âœ… All API calls logged
- âœ… Security events logged
- âœ… Performance metrics tracked
- âœ… Error events logged with context

---

## UX & Accessibility Standards

### WCAG 2.1 AA Compliance
- âœ… Perceivable: Color contrast, text alternatives, captions
- âœ… Operable: Keyboard accessible, no seizure triggers, enough time
- âœ… Understandable: Readable, predictable, input assistance
- âœ… Robust: Compatible, valid markup

### Design System
- âœ… Dark Emerald color palette (as provided)
- âœ… Day/night mode themes
- âœ… Responsive breakpoints
- âœ… Accessibility controls (high contrast, large font, reduced motion)

---

## Performance & Availability Requirements

### Performance Targets
- **Response Time**: < 2 seconds for standard operations
- **Question Generation**: < 5 seconds
- **Code Execution**: < 5 seconds
- **Feedback Generation**: < 3 seconds

### Availability Targets
- **Uptime**: 99.9% availability
- **Scheduled Maintenance**: Off-peak hours with notice
- **MTTR**: < 30 minutes for critical issues

### Scalability Targets
- **Concurrent Users**: 10,000+
- **Code Executions/sec**: 100+
- **Question Generations/day**: 1000+

---

## AI/Automation Enhancements

### Intelligent Question Generation
- AI analyzes learner history to personalize questions
- AI adapts difficulty based on performance
- AI ensures question variety and relevance

### Smart Feedback System
- AI identifies common error patterns
- AI provides targeted feedback based on error type
- AI learns from feedback effectiveness

### Cheating Detection
- AI pattern recognition for suspicious solutions
- AI similarity detection between submissions
- AI anomaly detection for unusual behavior

---

## Validation Checkpoint

### Requirements Completeness
âœ… All functional requirements documented  
âœ… All non-functional requirements specified  
âœ… User stories with acceptance criteria defined  
âœ… Business rules documented  
âœ… Integration requirements specified  
âœ… Compliance requirements identified  
âœ… UX and accessibility standards defined  
âœ… Performance targets established  

### TDD Readiness
âœ… All features have testable acceptance criteria  
âœ… Test cases identified for critical paths  
âœ… Mock requirements defined for external dependencies  

### Conflict Resolution
âœ… No conflicting requirements identified  
âœ… All requirements align with project objectives  
âœ… Integration requirements compatible  

---

**Document Status**: âœ… Complete - Ready for Architecture Design Phase  
**Next Phase**: System Architecture & Technical Design  
**Created**: Requirements Discovery & Analysis Phase


